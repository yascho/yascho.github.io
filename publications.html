<!doctype html>
<html lang="en">
  <head>
	<meta charset="utf-8" />
	<meta http-equiv="x-ua-compatible" content="ie=edge" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
      
	<title>
	  Yan Scholten
	</title>

	<!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Yan Scholten" />
<meta name="author" content="Yan Scholten" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Master&#39;s student in Informatics at the Technical University of Munich" />
<meta property="og:description" content="Master&#39;s student in Informatics at the Technical University of Munich" />
<link rel="canonical" href="https://yascho.github.io/publications" />
<meta property="og:url" content="https://yascho.github.io/publications" />
<meta property="og:site_name" content="Yan Scholten" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Yan Scholten" />
<meta name="twitter:site" content="@YanScholten" />
<meta name="twitter:creator" content="@Yan Scholten" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Yan Scholten"},"url":"https://yascho.github.io/publications","@type":"WebPage","description":"Master&#39;s student in Informatics at the Technical University of Munich","headline":"Yan Scholten","dateModified":"2023-10-11T11:52:31+02:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	<meta name="google-site-verification" content="chbllHIfU7hyPiWimLppN2ds-jk7A3FxoRpNI2xsg8Q" />
      
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
	<link rel="stylesheet" href="/css/main.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.6.1/font/bootstrap-icons.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link
	  href="https://fonts.googleapis.com/css?family=Open+Sans:400,300,700,800,600"
	  rel="stylesheet"
	  type="text/css"
	/>
	<link
	  href="https://fonts.googleapis.com/css?family=Muli:400,300"
	  rel="stylesheet"
	  type="text/css"
	/>
</head>
  <body class="d-flex flex-column h-100">
    <aside class="navbar-fixed-top navbar navbar-expand-lg navbar-light">
	<nav class="container row p-0">
		<div class="col ps-sm-2 ps-md-0">
			<div class="p-0">
				
					<h1 class="m-0 p-0 nav-heading">Yan Scholten</h1>
					<div class="m-0 p-0 text-muted authors d-none d-md-block d-xl-block">PhD student in Computer Science, Technical University of Munich</div>
				
			</div>
		</div>
		<div class="col-auto m-0 p-0">
			<!-- future responsive navbar button 
			<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
			<span class="navbar-toggler-icon"></span>
			</button>
			<div class="collapse navbar-collapse" id="navbarNav">-->
			<!--<h2 class="mt-5 pb-1">Yan Scholten</h2>-->
			<ul class="p-0 m-0">
				<ul class="navbar-nav">
				 
				<!--<li class="nav-item"><a aria-current="page" href="/publications">Publications</a></li> -->
					
						<li class="nav-item"><a aria-current="page" href="/">Home</a></li>
    				
				  
				<!--<li class="nav-item"><a aria-current="page" href="/publications">Publications</a></li> -->
					
      					<li class="nav-item"><a aria-current="page" href="/publications" class="active">Publications</a></li>
    				
				       
				<!--<li><a href="/blog">Blog</a></li>-->
			</ul>
			<!--</div>-->
		</div>
	</nav>
</aside>
    <main class="container flex-shrink-0 p-0">
      <article class="container"><h2 class="mt-5 pb-1">Publications</h2>
<div class="container mb-3">
  Up-to-date list on <a href="https://scholar.google.com/citations?hl=en&amp;user=8G2bJ7sAAAAJ&amp;sortby=pubdate" target="_blank" rel="noopener noreferrer">Google Scholar</a>.
</div>

<h3 class="text-center" id="headings">Conferences</h3>
<!-- 
<div class="container pe-0 ps-0 ps-md-3">
	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf">Hierarchical Randomized Smoothing</a>
<br>
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Jan Schuchardt,
	

	
		Aleksandar Bojchevski,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">Conference on Neural Information Processing Systems</span>, NeurIPS 2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2023hierarchical" aria-expanded="false"  aria-controls="scholten2023hierarchical">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/hierarchical-smoothing/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://nips.cc/virtual/2023/poster/72764" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/hierarchical_smoothing" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{scholten2023hierarchical,
  author = {Scholten, Yan and Schuchardt, Jan and Bojchevski, Aleksandar and Günnemann, Stephan},
  title = {Hierarchical Randomized Smoothing},
  booktitle = {Conference on Neural Information Processing Systems},
  abbr = {NeurIPS},
  year = {2023},
  slides = {https://yascho.github.io/assets/pdf/scholten2023hierarchical-slides.pdf},
  web = {https://www.cs.cit.tum.de/daml/hierarchical-smoothing/},
  code = {https://github.com/yascho/hierarchical_smoothing},
  talk = {https://nips.cc/virtual/2023/poster/72764},
  pdf = {https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2023hierarchical-poster.pdf},
  abs = {Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs -- by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both -- certifiably robust to perturbations and accurate.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2023hierarchical">
	<div class="card card-body">
		Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs – by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both – certifiably robust to perturbations and accurate.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://openreview.net/pdf?id=mLe63bAYc7">Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More</a>
<br>
<div class="container authors p-0">

	
		Jan Schuchardt,
	

	
		<b>Yan Scholten</b>,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">Conference on Neural Information Processing Systems</span>, NeurIPS 2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#schuchardt2023provable" aria-expanded="false"  aria-controls="schuchardt2023provable">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/equivariance-robustness/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://openreview.net/pdf?id=mLe63bAYc7" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://nips.cc/virtual/2023/poster/70555" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
	
		<a role="button" href="https://nips.cc/media/PosterPDFs/NeurIPS%202023/70555.png?t=1701529700.1135688" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
	
	<a role="button" onclick="var txt=`@inproceedings{schuchardt2023provable,
  author = {Schuchardt, Jan and Scholten, Yan and Günnemann, Stephan},
  title = {Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More},
  booktitle = {Conference on Neural Information Processing Systems},
  abbr = {NeurIPS},
  year = {2023},
  web = {https://www.cs.cit.tum.de/daml/equivariance-robustness/},
  poster = {https://nips.cc/media/PosterPDFs/NeurIPS%202023/70555.png?t=1701529700.1135688},
  talk = {https://nips.cc/virtual/2023/poster/70555},
  pdf = {https://openreview.net/pdf?id=mLe63bAYc7},
  abs = {A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="schuchardt2023provable">
	<div class="card card-body">
		A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input’s semantic content. Furthermore, there are perturbations for which a model’s prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task’s equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/abs/2310.04285">Assessing Robustness via Score-Based Adversarial Image Generation</a>
<br>
<div class="container authors p-0">

	
		Marcel Kollovieh,
	

	
		Lukas Gosch,
	

	
		<b>Yan Scholten</b>,
	

	
		Marten Lienen,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">arXiv preprint</span>,  2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#kollovieh2023assessing" aria-expanded="false"  aria-controls="kollovieh2023assessing">Abs</a>
	
	
	
		<a role="button" href="https://arxiv.org/abs/2310.04285" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
	
	<a role="button" onclick="var txt=`@inproceedings{kollovieh2023assessing,
  author = {Kollovieh, Marcel and Gosch, Lukas and Scholten, Yan and Lienen, Marten and Günnemann, Stephan},
  title = {Assessing Robustness via Score-Based Adversarial Image Generation},
  booktitle = {arXiv preprint},
  year = {2023},
  pdf = {https://arxiv.org/abs/2310.04285},
  abs = {Most adversarial attacks and defenses focus on perturbations within small l_p-norm constraints. However, l_p threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond l_p-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than l_p-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments. }
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="kollovieh2023assessing">
	<div class="card card-body">
		Most adversarial attacks and defenses focus on perturbations within small l_p-norm constraints. However, l_p threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond l_p-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than l_p-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments. 
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2022randomized.pdf">Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks</a>
<br>
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Jan Schuchardt,
	

	
		Simon Geisler,
	

	
		Aleksandar Bojchevski,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">Conference on Neural Information Processing Systems</span>, NeurIPS 2022.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2022randomized" aria-expanded="false"  aria-controls="scholten2022randomized">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/interception-smoothing/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://www.youtube.com/watch?v=rbFiXrh9Snk" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/interception_smoothing" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{scholten2022randomized,
  author = {Scholten, Yan and Schuchardt, Jan and Geisler, Simon and Bojchevski, Aleksandar and Günnemann, Stephan},
  title = {Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks},
  booktitle = {Conference on Neural Information Processing Systems},
  abbr = {NeurIPS},
  year = {2022},
  pdf = {https://yascho.github.io/assets/pdf/scholten2022randomized.pdf},
  code = {https://github.com/yascho/interception_smoothing},
  talk = {https://www.youtube.com/watch?v=rbFiXrh9Snk},
  slides = {https://yascho.github.io/assets/pdf/scholten2022randomized-slides.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2022randomized-poster.pdf},
  web = {https://www.cs.cit.tum.de/daml/interception-smoothing/},
  abs = {Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2022randomized">
	<div class="card card-body">
		Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf">CauseNet: Towards a Causality Graph Extracted from the Web</a>
<br>
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Henning Wachsmuth,
	

	
		Axel-Cyrille Ngonga Ngomo,
	

	
		and Martin Potthast
	

<br>
<span class="font-italic">International Conference on Information and Knowledge Management</span>, CIKM 2020.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#heindorf2020causenet" aria-expanded="false"  aria-controls="heindorf2020causenet">Abs</a>
	
	
		<a role="button" href="https://causenet.org/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://www.youtube.com/watch?v=aza77qJeZBo" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
	
	
		<a role="button" href="https://github.com/causenet-org/CIKM-20" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{heindorf2020causenet,
  author = {Heindorf, Stefan and Scholten, Yan and Wachsmuth, Henning and Ngomo, Axel{-}Cyrille Ngonga and Potthast, Martin},
  title = {CauseNet: Towards a Causality Graph Extracted from the Web},
  booktitle = {International Conference on Information and Knowledge Management},
  abbr = {CIKM},
  pages = {3023--3030},
  publisher = {{ACM}},
  year = {2020},
  web = {https://causenet.org/},
  pdf = {https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf},
  code = {https://github.com/causenet-org/CIKM-20},
  talk = {https://www.youtube.com/watch?v=aza77qJeZBo},
  abs = {Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2020causenet">
	<div class="card card-body">
		Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://webis.de/downloads/publications/papers/heindorf_2019.pdf">Debiasing Vandalism Detection Models at Wikidata</a>
<br>
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Gregor Engels,
	

	
		and Martin Potthast
	

<br>
<span class="font-italic">International World Wide Web Conference</span>, WWW 2019.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#heindorf2019debiasing" aria-expanded="false"  aria-controls="heindorf2019debiasing">Abs</a>
	
	
		<a role="button" href="https://www.heindorf.me/wdvd.html" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://webis.de/downloads/publications/papers/heindorf_2019.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
		<a role="button" href="https://github.com/heindorf/www19-fair-classification" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{heindorf2019debiasing,
  author = {Heindorf, Stefan and Scholten, Yan and Engels, Gregor and Potthast, Martin},
  title = {Debiasing Vandalism Detection Models at Wikidata},
  booktitle = {International World Wide Web Conference},
  abbr = {WWW},
  pages = {670--680},
  publisher = {{ACM}},
  year = {2019},
  web = {https://www.heindorf.me/wdvd.html},
  pdf = {https://webis.de/downloads/publications/papers/heindorf_2019.pdf},
  code = {https://github.com/heindorf/www19-fair-classification},
  abs = {Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC AUC and 0.316 PR AUC.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2019debiasing">
	<div class="card card-body">
		Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC AUC and 0.316 PR AUC.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://dl.gi.de/bitstream/handle/20.500.12116/24997/paper3_25.pdf">Debiasing Vandalism Detection Models at Wikidata (Extended Abstract)</a>
<br>
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Gregor Engels,
	

	
		and Martin Potthast
	

<br>
<span class="font-italic">Jahrestagung der Gesellschaft für Informatik e.V.</span>, INFORMATIK 2019.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	
		<a role="button" href="https://dl.gi.de/handle/20.500.12116/24997" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://dl.gi.de/bitstream/handle/20.500.12116/24997/paper3_25.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
		<a role="button" href="https://github.com/heindorf/www19-fair-classification" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{heindorf2019debiasingextended,
  author = {Heindorf, Stefan and Scholten, Yan and Engels, Gregor and Potthast, Martin},
  title = {Debiasing Vandalism Detection Models at Wikidata (Extended Abstract)},
  booktitle = {Jahrestagung der Gesellschaft für Informatik e.V.},
  abbr = {INFORMATIK},
  series = {{LNI}},
  volume = {{P-294}},
  pages = {289--290},
  publisher = {{GI}},
  year = {2019},
  web = {https://dl.gi.de/handle/20.500.12116/24997},
  pdf = {https://dl.gi.de/bitstream/handle/20.500.12116/24997/paper3_25.pdf},
  code = {https://github.com/heindorf/www19-fair-classification}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2019debiasingextended">
	<div class="card card-body">
		
	</div>
</div>
</div></li></ol>
</div>
-->

<div class="container pe-0 ps-0 ps-md-3">

  <h2>2023</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf">Hierarchical Randomized Smoothing</a>
<br />
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Jan Schuchardt,
	

	
		Aleksandar Bojchevski,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">Conference on Neural Information Processing Systems</span>, NeurIPS 2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2023hierarchical" aria-expanded="false" aria-controls="scholten2023hierarchical">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/hierarchical-smoothing/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://nips.cc/virtual/2023/poster/72764" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/hierarchical_smoothing" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{scholten2023hierarchical,
  author = {Scholten, Yan and Schuchardt, Jan and Bojchevski, Aleksandar and Günnemann, Stephan},
  title = {Hierarchical Randomized Smoothing},
  booktitle = {Conference on Neural Information Processing Systems},
  abbr = {NeurIPS},
  year = {2023},
  slides = {https://yascho.github.io/assets/pdf/scholten2023hierarchical-slides.pdf},
  web = {https://www.cs.cit.tum.de/daml/hierarchical-smoothing/},
  code = {https://github.com/yascho/hierarchical_smoothing},
  talk = {https://nips.cc/virtual/2023/poster/72764},
  pdf = {https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2023hierarchical-poster.pdf},
  abs = {Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs -- by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both -- certifiably robust to perturbations and accurate.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2023hierarchical">
	<div class="card card-body">
		Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs – by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both – certifiably robust to perturbations and accurate.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://openreview.net/pdf?id=mLe63bAYc7">Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More</a>
<br />
<div class="container authors p-0">

	
		Jan Schuchardt,
	

	
		<b>Yan Scholten</b>,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">Conference on Neural Information Processing Systems</span>, NeurIPS 2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#schuchardt2023provable" aria-expanded="false" aria-controls="schuchardt2023provable">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/equivariance-robustness/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://openreview.net/pdf?id=mLe63bAYc7" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://nips.cc/virtual/2023/poster/70555" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
	
		<a role="button" href="https://nips.cc/media/PosterPDFs/NeurIPS%202023/70555.png?t=1701529700.1135688" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
	
	<a role="button" onclick="var txt=`@inproceedings{schuchardt2023provable,
  author = {Schuchardt, Jan and Scholten, Yan and Günnemann, Stephan},
  title = {Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More},
  booktitle = {Conference on Neural Information Processing Systems},
  abbr = {NeurIPS},
  year = {2023},
  web = {https://www.cs.cit.tum.de/daml/equivariance-robustness/},
  poster = {https://nips.cc/media/PosterPDFs/NeurIPS%202023/70555.png?t=1701529700.1135688},
  talk = {https://nips.cc/virtual/2023/poster/70555},
  pdf = {https://openreview.net/pdf?id=mLe63bAYc7},
  abs = {A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="schuchardt2023provable">
	<div class="card card-body">
		A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input’s semantic content. Furthermore, there are perturbations for which a model’s prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task’s equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/abs/2310.04285">Assessing Robustness via Score-Based Adversarial Image Generation</a>
<br />
<div class="container authors p-0">

	
		Marcel Kollovieh,
	

	
		Lukas Gosch,
	

	
		<b>Yan Scholten</b>,
	

	
		Marten Lienen,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">arXiv preprint</span>,  2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#kollovieh2023assessing" aria-expanded="false" aria-controls="kollovieh2023assessing">Abs</a>
	
	
	
		<a role="button" href="https://arxiv.org/abs/2310.04285" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
	
	<a role="button" onclick="var txt=`@inproceedings{kollovieh2023assessing,
  author = {Kollovieh, Marcel and Gosch, Lukas and Scholten, Yan and Lienen, Marten and Günnemann, Stephan},
  title = {Assessing Robustness via Score-Based Adversarial Image Generation},
  booktitle = {arXiv preprint},
  year = {2023},
  pdf = {https://arxiv.org/abs/2310.04285},
  abs = {Most adversarial attacks and defenses focus on perturbations within small l_p-norm constraints. However, l_p threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond l_p-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than l_p-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments. }
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="kollovieh2023assessing">
	<div class="card card-body">
		Most adversarial attacks and defenses focus on perturbations within small l_p-norm constraints. However, l_p threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond l_p-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than l_p-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments. 
	</div>
</div>
</div></li></ol>
  </div>

  <h2>2022</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2022randomized.pdf">Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks</a>
<br />
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Jan Schuchardt,
	

	
		Simon Geisler,
	

	
		Aleksandar Bojchevski,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">Conference on Neural Information Processing Systems</span>, NeurIPS 2022.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2022randomized" aria-expanded="false" aria-controls="scholten2022randomized">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/interception-smoothing/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://www.youtube.com/watch?v=rbFiXrh9Snk" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/interception_smoothing" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{scholten2022randomized,
  author = {Scholten, Yan and Schuchardt, Jan and Geisler, Simon and Bojchevski, Aleksandar and Günnemann, Stephan},
  title = {Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks},
  booktitle = {Conference on Neural Information Processing Systems},
  abbr = {NeurIPS},
  year = {2022},
  pdf = {https://yascho.github.io/assets/pdf/scholten2022randomized.pdf},
  code = {https://github.com/yascho/interception_smoothing},
  talk = {https://www.youtube.com/watch?v=rbFiXrh9Snk},
  slides = {https://yascho.github.io/assets/pdf/scholten2022randomized-slides.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2022randomized-poster.pdf},
  web = {https://www.cs.cit.tum.de/daml/interception-smoothing/},
  abs = {Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2022randomized">
	<div class="card card-body">
		Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.
	</div>
</div>
</div></li></ol>
  </div>

  <h2>2020</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf">CauseNet: Towards a Causality Graph Extracted from the Web</a>
<br />
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Henning Wachsmuth,
	

	
		Axel-Cyrille Ngonga Ngomo,
	

	
		and Martin Potthast
	

<br />
<span class="font-italic">International Conference on Information and Knowledge Management</span>, CIKM 2020.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#heindorf2020causenet" aria-expanded="false" aria-controls="heindorf2020causenet">Abs</a>
	
	
		<a role="button" href="https://causenet.org/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://www.youtube.com/watch?v=aza77qJeZBo" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
	
	
		<a role="button" href="https://github.com/causenet-org/CIKM-20" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{heindorf2020causenet,
  author = {Heindorf, Stefan and Scholten, Yan and Wachsmuth, Henning and Ngomo, Axel{-}Cyrille Ngonga and Potthast, Martin},
  title = {CauseNet: Towards a Causality Graph Extracted from the Web},
  booktitle = {International Conference on Information and Knowledge Management},
  abbr = {CIKM},
  pages = {3023--3030},
  publisher = {{ACM}},
  year = {2020},
  web = {https://causenet.org/},
  pdf = {https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf},
  code = {https://github.com/causenet-org/CIKM-20},
  talk = {https://www.youtube.com/watch?v=aza77qJeZBo},
  abs = {Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2020causenet">
	<div class="card card-body">
		Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.
	</div>
</div>
</div></li></ol>
  </div>

  <h2>2019</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://webis.de/downloads/publications/papers/heindorf_2019.pdf">Debiasing Vandalism Detection Models at Wikidata</a>
<br />
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Gregor Engels,
	

	
		and Martin Potthast
	

<br />
<span class="font-italic">International World Wide Web Conference</span>, WWW 2019.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#heindorf2019debiasing" aria-expanded="false" aria-controls="heindorf2019debiasing">Abs</a>
	
	
		<a role="button" href="https://www.heindorf.me/wdvd.html" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://webis.de/downloads/publications/papers/heindorf_2019.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
		<a role="button" href="https://github.com/heindorf/www19-fair-classification" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{heindorf2019debiasing,
  author = {Heindorf, Stefan and Scholten, Yan and Engels, Gregor and Potthast, Martin},
  title = {Debiasing Vandalism Detection Models at Wikidata},
  booktitle = {International World Wide Web Conference},
  abbr = {WWW},
  pages = {670--680},
  publisher = {{ACM}},
  year = {2019},
  web = {https://www.heindorf.me/wdvd.html},
  pdf = {https://webis.de/downloads/publications/papers/heindorf_2019.pdf},
  code = {https://github.com/heindorf/www19-fair-classification},
  abs = {Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC AUC and 0.316 PR AUC.}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2019debiasing">
	<div class="card card-body">
		Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC AUC and 0.316 PR AUC.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://dl.gi.de/bitstream/handle/20.500.12116/24997/paper3_25.pdf">Debiasing Vandalism Detection Models at Wikidata (Extended Abstract)</a>
<br />
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Gregor Engels,
	

	
		and Martin Potthast
	

<br />
<span class="font-italic">Jahrestagung der Gesellschaft für Informatik e.V.</span>, INFORMATIK 2019.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	
		<a role="button" href="https://dl.gi.de/handle/20.500.12116/24997" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://dl.gi.de/bitstream/handle/20.500.12116/24997/paper3_25.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
		<a role="button" href="https://github.com/heindorf/www19-fair-classification" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
	<a role="button" onclick="var txt=`@inproceedings{heindorf2019debiasingextended,
  author = {Heindorf, Stefan and Scholten, Yan and Engels, Gregor and Potthast, Martin},
  title = {Debiasing Vandalism Detection Models at Wikidata (Extended Abstract)},
  booktitle = {Jahrestagung der Gesellschaft für Informatik e.V.},
  abbr = {INFORMATIK},
  series = {{LNI}},
  volume = {{P-294}},
  pages = {289--290},
  publisher = {{GI}},
  year = {2019},
  web = {https://dl.gi.de/handle/20.500.12116/24997},
  pdf = {https://dl.gi.de/bitstream/handle/20.500.12116/24997/paper3_25.pdf},
  code = {https://github.com/heindorf/www19-fair-classification}
}
`; copy(txt)" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2019debiasingextended">
	<div class="card card-body">
		
	</div>
</div>
</div></li></ol>
  </div>

</div>
</article>
    </main>
	  <footer class="footer m-auto mt-5 p-0">
	<div class="d-flex p-0">
		<div class="flex-fill text-start p-0">
			<span class="text-muted">Yan Scholten &copy; <script>document.write(/\d{4}/.exec(Date())[0])</script>
</span>
		</div>
		<div class="flex-fill text-end p-0">
			<span class="text-muted">Last updated: October 11, 2023.</span>
		</div>
	</div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    <script src="/js/copy.js"></script>
    <script>
      $(function () {
      $('[data-toggle="tooltip"]').tooltip({
      animated: 'fade',
      trigger: 'click',
      });
    });
    $("[data-toggle='tooltip']").on('mouseleave', function(){
      setTimeout(function() {   //calls click event after a certain time
       $('[data-toggle="tooltip"]').tooltip('hide');
      }, 1000);
    });
    </script>
  </body>
</html>