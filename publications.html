<!doctype html>
<html lang="en">
  <head>
	<meta charset="utf-8" />
	<meta http-equiv="x-ua-compatible" content="ie=edge" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
      
	<title>
	  Yan Scholten
	</title>

	<!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Yan Scholten" />
<meta name="author" content="Yan Scholten" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Master&#39;s student in Informatics at the Technical University of Munich" />
<meta property="og:description" content="Master&#39;s student in Informatics at the Technical University of Munich" />
<link rel="canonical" href="https://yascho.github.io/publications" />
<meta property="og:url" content="https://yascho.github.io/publications" />
<meta property="og:site_name" content="Yan Scholten" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Yan Scholten" />
<meta name="twitter:site" content="@YanScholten" />
<meta name="twitter:creator" content="@Yan Scholten" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Yan Scholten"},"url":"https://yascho.github.io/publications","@type":"WebPage","description":"Master&#39;s student in Informatics at the Technical University of Munich","headline":"Yan Scholten","dateModified":"2025-01-22T20:38:42+01:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	<meta name="google-site-verification" content="chbllHIfU7hyPiWimLppN2ds-jk7A3FxoRpNI2xsg8Q" />
      
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
	<link rel="stylesheet" href="/css/main.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.6.1/font/bootstrap-icons.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link
	  href="https://fonts.googleapis.com/css?family=Open+Sans:400,300,700,800,600"
	  rel="stylesheet"
	  type="text/css"
	/>
	<link
	  href="https://fonts.googleapis.com/css?family=Muli:400,300"
	  rel="stylesheet"
	  type="text/css"
	/>
</head>
  <body class="d-flex flex-column h-100">
    <aside class="navbar-fixed-top navbar navbar-expand-lg navbar-light">
	<nav class="container row p-0">
		<div class="col ps-sm-2 ps-md-0">
			<div class="p-0">
				
					<h1 class="m-0 p-0 nav-heading">Yan Scholten</h1>
					<div class="m-0 p-0 text-muted authors d-none d-md-block d-xl-block">PhD student in Computer Science, Technical University of Munich</div>
				
			</div>
		</div>
		<div class="col-auto m-0 p-0">
			<!-- future responsive navbar button 
			<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
			<span class="navbar-toggler-icon"></span>
			</button>
			<div class="collapse navbar-collapse" id="navbarNav">-->
			<!--<h2 class="mt-5 pb-1">Yan Scholten</h2>-->
			<ul class="p-0 m-0">
				<ul class="navbar-nav">
				 
				<!--<li class="nav-item"><a aria-current="page" href="/publications">Publications</a></li> -->
					
						<li class="nav-item"><a aria-current="page" href="/">Home</a></li>
    				
				  
				<!--<li class="nav-item"><a aria-current="page" href="/publications">Publications</a></li> -->
					
      					<li class="nav-item"><a aria-current="page" href="/publications" class="active">Publications</a></li>
    				
				       
				<!--<li><a href="/blog">Blog</a></li>-->
			</ul>
			<!--</div>-->
		</div>
	</nav>
</aside>
    <main class="container flex-shrink-0 p-0">
      <article class="container"><h2 class="mt-5 pb-1">Publications</h2>
<div class="container mb-3">
  Up-to-date list on <a href="https://scholar.google.com/citations?hl=en&amp;user=8G2bJ7sAAAAJ&amp;sortby=pubdate" target="_blank" rel="noopener noreferrer">Google Scholar</a>.
</div>

<!--<h3 class="text-center" id="headings">Conferences</h3>-->
<!-- 
<div class="container pe-0 ps-0 ps-md-3">
	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/pdf/2507.04219">Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs</a>
<br>
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Sophie Xhonneux,
	

	
		Leo Schwinn*,
	

	
		and Stephan Günnemann*
	

<br>
<span class="font-italic">arXiv preprint</span>,  2025.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2025model" aria-expanded="false"  aria-controls="scholten2025model">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/partial-model-collapse/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://arxiv.org/pdf/2507.04219" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
		<a role="button" href="https://github.com/yascho/partial-model-collapse" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@misc{scholten2025model,
  title = {Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs},
  author = {Scholten, Yan and Xhonneux, Sophie and Schwinn*, Leo and Günnemann*, Stephan},
  year = {2025},
  eprint = {2507.04219},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
  url = {https://arxiv.org/abs/2507.04219},
  pdf = {https://arxiv.org/pdf/2507.04219},
  organization = {arXiv preprint},
  web = {https://www.cs.cit.tum.de/daml/partial-model-collapse/},
  code = {https://github.com/yascho/partial-model-collapse},
  abs = {Current unlearning methods for LLMs optimize on the private information they seek to remove by incorporating it into their training objectives. We argue this not only risks reinforcing exposure to sensitive data, it also fundamentally contradicts the principle of minimizing its use. As a remedy, we propose a novel unlearning method - Partial Model Collapse (PMC), which does not require unlearning targets in the unlearning objective. Our approach is inspired by recent observations that training generative models on their own generations leads to distribution collapse, effectively removing information from the model. Our core idea is to leverage this collapse for unlearning by triggering collapse partially on the sensitive data. We theoretically analyze that our approach converges to the desired outcome, i.e. the LLM unlearns the information in the forget set. We empirically demonstrate that PMC overcomes two key limitations of existing unlearning approaches that explicitly optimize on unlearning targets, and more effectively removes private information from model outputs. Overall, our contributions represent an important step toward more comprehensive unlearning that aligns with real-world privacy constraints.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2025model">
	<div class="card card-body">
		Current unlearning methods for LLMs optimize on the private information they seek to remove by incorporating it into their training objectives. We argue this not only risks reinforcing exposure to sensitive data, it also fundamentally contradicts the principle of minimizing its use. As a remedy, we propose a novel unlearning method - Partial Model Collapse (PMC), which does not require unlearning targets in the unlearning objective. Our approach is inspired by recent observations that training generative models on their own generations leads to distribution collapse, effectively removing information from the model. Our core idea is to leverage this collapse for unlearning by triggering collapse partially on the sensitive data. We theoretically analyze that our approach converges to the desired outcome, i.e. the LLM unlearns the information in the forget set. We empirically demonstrate that PMC overcomes two key limitations of existing unlearning approaches that explicitly optimize on unlearning targets, and more effectively removes private information from model outputs. Overall, our contributions represent an important step toward more comprehensive unlearning that aligns with real-world privacy constraints.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/pdf/2507.04446">Tail-aware Adversarial Attacks: A Distributional Approach to Efficient LLM Jailbreaking</a>
<br>
<div class="container authors p-0">

	
		Tim Beyer,
	

	
		<b>Yan Scholten</b>,
	

	
		Leo Schwinn*,
	

	
		and Stephan Günnemann*
	

<br>
<span class="font-italic">arXiv preprint</span>,  2025.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#beyer2025tailawareadversarialattacksdistributional" aria-expanded="false"  aria-controls="beyer2025tailawareadversarialattacksdistributional">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/tail-aware-attacks/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://arxiv.org/pdf/2507.04446" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
	
  	<a role="button" onclick="var txt=`@misc{beyer2025tailawareadversarialattacksdistributional,
  title = {Tail-aware Adversarial Attacks: A Distributional Approach to Efficient LLM Jailbreaking},
  author = {Beyer, Tim and Scholten, Yan and Schwinn*, Leo and Günnemann*, Stephan},
  year = {2025},
  eprint = {2507.04446},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
  url = {https://arxiv.org/abs/2507.04446},
  pdf = {https://arxiv.org/pdf/2507.04446},
  web = {https://www.cs.cit.tum.de/daml/tail-aware-attacks/},
  organization = {arXiv preprint},
  abs = {To guarantee safe and robust deployment of large language models (LLMs) at scale, it is critical to accurately assess their adversarial robustness. Existing adversarial attacks typically target harmful responses in single-point, greedy generations, overlooking the inherently stochastic nature of LLMs. In this paper, we propose a novel framework for adversarial robustness evaluation that explicitly models the entire output distribution, including tail-risks, providing better estimates for model robustness at scale. By casting the attack process as a resource allocation problem between optimization and sampling, we determine compute-optimal tradeoffs and show that integrating sampling into existing attacks boosts ASR by up to 48% and improves efficiency by up to two orders of magnitude. Our framework also enables us to analyze how different attack algorithms affect output harm distributions. Surprisingly, we find that most optimization strategies have little effect on output harmfulness. Finally, we introduce a data-free proof-of-concept objective based on entropy-maximization to demonstrate how our tail-aware perspective enables new optimization targets. Overall, our findings highlight the importance of tail-aware attacks and evaluation protocols to accurately assess and strengthen LLM safety.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="beyer2025tailawareadversarialattacksdistributional">
	<div class="card card-body">
		To guarantee safe and robust deployment of large language models (LLMs) at scale, it is critical to accurately assess their adversarial robustness. Existing adversarial attacks typically target harmful responses in single-point, greedy generations, overlooking the inherently stochastic nature of LLMs. In this paper, we propose a novel framework for adversarial robustness evaluation that explicitly models the entire output distribution, including tail-risks, providing better estimates for model robustness at scale. By casting the attack process as a resource allocation problem between optimization and sampling, we determine compute-optimal tradeoffs and show that integrating sampling into existing attacks boosts ASR by up to 48% and improves efficiency by up to two orders of magnitude. Our framework also enables us to analyze how different attack algorithms affect output harm distributions. Surprisingly, we find that most optimization strategies have little effect on output harmfulness. Finally, we introduce a data-free proof-of-concept objective based on entropy-maximization to demonstrate how our tail-aware perspective enables new optimization targets. Overall, our findings highlight the importance of tail-aware attacks and evaluation protocols to accurately assess and strengthen LLM safety.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2025probabilistic.pdf">A Probabilistic Perspective on Unlearning and Alignment for Large Language Models</a>
<br>
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Stephan Günnemann,
	

	
		and Leo Schwinn
	

<br>
<span class="font-italic">International Conference on Learning Representations</span>, ICLR 2025 (<strong class="acceptance_type">Oral</strong>).
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2025probabilistic" aria-expanded="false"  aria-controls="scholten2025probabilistic">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/probabilistic-unlearning/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025probabilistic.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025probabilistic-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025probabilistic-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/probabilistic-unlearning" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{scholten2025probabilistic,
  author = {Scholten, Yan and Günnemann, Stephan and Schwinn, Leo},
  title = {A Probabilistic Perspective on Unlearning and Alignment for Large Language Models},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  year = {2025},
  url = {https://openreview.net/forum?id=51WraMid8K},
  organization = {International Conference on Learning Representations},
  abbr = {ICLR},
  web = {https://www.cs.cit.tum.de/daml/probabilistic-unlearning/},
  pdf = {https://yascho.github.io/assets/pdf/scholten2025probabilistic.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2025probabilistic-poster.pdf},
  slides = {https://yascho.github.io/assets/pdf/scholten2025probabilistic-slides.pdf},
  code = {https://github.com/yascho/probabilistic-unlearning},
  abs = {Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding inaccurate estimations of model capabilities. This is particularly problematic in critical contexts such as unlearning and alignment, where precise model evaluations are crucial. To remedy this, we introduce the first formal probabilistic evaluation framework for LLMs. Namely, we propose novel metrics with high probability guarantees concerning the output distribution of a model. Our metrics are application-independent and allow practitioners to make more reliable estimates about model capabilities before deployment. Our experimental analysis reveals that deterministic evaluations falsely indicate successful unlearning and alignment, whereas our probabilistic evaluations better capture model capabilities. We show how to overcome challenges associated with probabilistic outputs in a case study on unlearning by introducing (1) a novel loss based on entropy optimization, and (2) adaptive temperature scaling. We demonstrate that our approach significantly enhances unlearning in probabilistic settings on recent benchmarks. Overall, our proposed shift from point estimates to probabilistic evaluations of output distributions represents an important step toward comprehensive evaluations of LLMs.},
  howpublished = {Oral}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2025probabilistic">
	<div class="card card-body">
		Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding inaccurate estimations of model capabilities. This is particularly problematic in critical contexts such as unlearning and alignment, where precise model evaluations are crucial. To remedy this, we introduce the first formal probabilistic evaluation framework for LLMs. Namely, we propose novel metrics with high probability guarantees concerning the output distribution of a model. Our metrics are application-independent and allow practitioners to make more reliable estimates about model capabilities before deployment. Our experimental analysis reveals that deterministic evaluations falsely indicate successful unlearning and alignment, whereas our probabilistic evaluations better capture model capabilities. We show how to overcome challenges associated with probabilistic outputs in a case study on unlearning by introducing (1) a novel loss based on entropy optimization, and (2) adaptive temperature scaling. We demonstrate that our approach significantly enhances unlearning in probabilistic settings on recent benchmarks. Overall, our proposed shift from point estimates to probabilistic evaluations of output distributions represents an important step toward comprehensive evaluations of LLMs.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2025provably.pdf">Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning</a>
<br>
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">International Conference on Learning Representations</span>, ICLR 2025 (<strong class="acceptance_type">Spotlight</strong>).
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2025provably" aria-expanded="false"  aria-controls="scholten2025provably">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/reliable-conformal-prediction/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025provably.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025provably-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/reliable_conformal_prediction" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{scholten2025provably,
  author = {Scholten, Yan and Günnemann, Stephan},
  title = {Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  year = {2025},
  url = {https://openreview.net/forum?id=ofuLWn8DFZ},
  organization = {International Conference on Learning Representations},
  abbr = {ICLR},
  pdf = {https://yascho.github.io/assets/pdf/scholten2025provably.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2025provably-poster.pdf},
  web = {https://www.cs.cit.tum.de/daml/reliable-conformal-prediction/},
  code = {https://github.com/yascho/reliable_conformal_prediction},
  abs = {Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning.},
  howpublished = {Spotlight}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2025provably">
	<div class="card card-body">
		Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/pdf/2502.11910">Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives</a>
<br>
<div class="container authors p-0">

	
		Leo Schwinn,
	

	
		<b>Yan Scholten</b>,
	

	
		Tom Wollschläger,
	

	
		Sophie Xhonneux,
	

	
		Stephen Casper,
	

	
		Stephan Günnemann,
	

	
		and Gauthier Gidel
	

<br>
<span class="font-italic">arXiv preprint</span>,  2025.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#schwinn2025adversarial" aria-expanded="false"  aria-controls="schwinn2025adversarial">Abs</a>
	
	
	
		<a role="button" href="https://arxiv.org/pdf/2502.11910" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
	
  	<a role="button" onclick="var txt=`@misc{schwinn2025adversarial,
  title = {Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives},
  author = {Schwinn, Leo and Scholten, Yan and Wollschläger, Tom and Xhonneux, Sophie and Casper, Stephen and Günnemann, Stephan and Gidel, Gauthier},
  year = {2025},
  eprint = {2502.11910},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
  url = {https://arxiv.org/abs/2502.11910},
  pdf = {https://arxiv.org/pdf/2502.11910},
  organization = {arXiv preprint},
  abs = {Misaligned research objectives have considerably hindered progress in adversarial robustness research over the past decade. For instance, an extensive focus on optimizing target metrics, while neglecting rigorous standardized evaluation, has led researchers to pursue ad-hoc heuristic defenses that were seemingly effective. Yet, most of these were exposed as flawed by subsequent evaluations, ultimately contributing little measurable progress to the field. In this position paper, we illustrate that current research on the robustness of large language models (LLMs) risks repeating past patterns with potentially worsened real-world implications. To address this, we argue that realigned objectives are necessary for meaningful progress in adversarial alignment. To this end, we build on established cybersecurity taxonomy to formally define differences between past and emerging threat models that apply to LLMs. Using this framework, we illustrate that progress requires disentangling adversarial alignment into addressable sub-problems and returning to core academic principles, such as measureability, reproducibility, and comparability. Although the field presents significant challenges, the fresh start on adversarial robustness offers the unique opportunity to build on past experience while avoiding previous mistakes.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="schwinn2025adversarial">
	<div class="card card-body">
		Misaligned research objectives have considerably hindered progress in adversarial robustness research over the past decade. For instance, an extensive focus on optimizing target metrics, while neglecting rigorous standardized evaluation, has led researchers to pursue ad-hoc heuristic defenses that were seemingly effective. Yet, most of these were exposed as flawed by subsequent evaluations, ultimately contributing little measurable progress to the field. In this position paper, we illustrate that current research on the robustness of large language models (LLMs) risks repeating past patterns with potentially worsened real-world implications. To address this, we argue that realigned objectives are necessary for meaningful progress in adversarial alignment. To this end, we build on established cybersecurity taxonomy to formally define differences between past and emerging threat models that apply to LLMs. Using this framework, we illustrate that progress requires disentangling adversarial alignment into addressable sub-problems and returning to core academic principles, such as measureability, reproducibility, and comparability. Although the field presents significant challenges, the fresh start on adversarial robustness offers the unique opportunity to build on past experience while avoiding previous mistakes.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf">Hierarchical Randomized Smoothing</a>
<br>
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Jan Schuchardt,
	

	
		Aleksandar Bojchevski,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">Advances in Neural Information Processing Systems</span>, NeurIPS 2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2023hierarchical" aria-expanded="false"  aria-controls="scholten2023hierarchical">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/hierarchical-smoothing/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://nips.cc/virtual/2023/poster/72764" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/hierarchical_smoothing" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{scholten2023hierarchical,
  author = {Scholten, Yan and Schuchardt, Jan and Bojchevski, Aleksandar and Günnemann, Stephan},
  title = {Hierarchical Randomized Smoothing},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year = {2023},
  url = {https://openreview.net/forum?id=6IhNHKyuJO},
  organization = {Advances in Neural Information Processing Systems},
  abbr = {NeurIPS},
  slides = {https://yascho.github.io/assets/pdf/scholten2023hierarchical-slides.pdf},
  web = {https://www.cs.cit.tum.de/daml/hierarchical-smoothing/},
  code = {https://github.com/yascho/hierarchical_smoothing},
  talk = {https://nips.cc/virtual/2023/poster/72764},
  pdf = {https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2023hierarchical-poster.pdf},
  abs = {Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs -- by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both -- certifiably robust to perturbations and accurate.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2023hierarchical">
	<div class="card card-body">
		Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs – by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both – certifiably robust to perturbations and accurate.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://openreview.net/pdf?id=mLe63bAYc7">Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More</a>
<br>
<div class="container authors p-0">

	
		Jan Schuchardt,
	

	
		<b>Yan Scholten</b>,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">Advances in Neural Information Processing Systems</span>, NeurIPS 2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#schuchardt2023provable" aria-expanded="false"  aria-controls="schuchardt2023provable">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/equivariance-robustness/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://openreview.net/pdf?id=mLe63bAYc7" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://nips.cc/virtual/2023/poster/70555" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
	
		<a role="button" href="https://nips.cc/media/PosterPDFs/NeurIPS%202023/70555.png?t=1701529700.1135688" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/jan-schuchardt/group_equivariance_robustness" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{schuchardt2023provable,
  author = {Schuchardt, Jan and Scholten, Yan and Günnemann, Stephan},
  title = {Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year = {2023},
  url = {https://openreview.net/forum?id=mLe63bAYc7},
  organization = {Advances in Neural Information Processing Systems},
  abbr = {NeurIPS},
  web = {https://www.cs.cit.tum.de/daml/equivariance-robustness/},
  poster = {https://nips.cc/media/PosterPDFs/NeurIPS%202023/70555.png?t=1701529700.1135688},
  talk = {https://nips.cc/virtual/2023/poster/70555},
  pdf = {https://openreview.net/pdf?id=mLe63bAYc7},
  code = {https://github.com/jan-schuchardt/group_equivariance_robustness},
  abs = {A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="schuchardt2023provable">
	<div class="card card-body">
		A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input’s semantic content. Furthermore, there are perturbations for which a model’s prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task’s equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/abs/2310.04285">Assessing Robustness via Score-Based Adversarial Image Generation</a>
<br>
<div class="container authors p-0">

	
		Marcel Kollovieh,
	

	
		Lukas Gosch,
	

	
		Marten Lienen,
	

	
		<b>Yan Scholten</b>,
	

	
		Leo Schwinn,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">Transactions on Machine Learning Research</span>, TMLR 2024.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#kollovieh2024assessing" aria-expanded="false"  aria-controls="kollovieh2024assessing">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/scoreag/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://arxiv.org/abs/2310.04285" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
	
  	<a role="button" onclick="var txt=`@inproceedings{kollovieh2024assessing,
  title = {Assessing Robustness via Score-Based Adversarial Image Generation},
  author = {Kollovieh, Marcel and Gosch, Lukas and Lienen, Marten and Scholten, Yan and Schwinn, Leo and Günnemann, Stephan},
  booktitle = {Transactions on Machine Learning Research},
  organization = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  year = {2024},
  url = {https://openreview.net/forum?id=7Oqb6zlGWl},
  abbr = {TMLR},
  pdf = {https://arxiv.org/abs/2310.04285},
  web = {https://www.cs.cit.tum.de/daml/scoreag/},
  abs = {Most adversarial attacks and defenses focus on perturbations within small lp-norm constraints. However, lp threat models cannot capture all relevant semantics-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate unrestricted adversarial examples that overcome the limitations of lp-norm constraints. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG improves upon the majority of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than lp-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="kollovieh2024assessing">
	<div class="card card-body">
		Most adversarial attacks and defenses focus on perturbations within small lp-norm constraints. However, lp threat models cannot capture all relevant semantics-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate unrestricted adversarial examples that overcome the limitations of lp-norm constraints. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG improves upon the majority of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than lp-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2022randomized.pdf">Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks</a>
<br>
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Jan Schuchardt,
	

	
		Simon Geisler,
	

	
		Aleksandar Bojchevski,
	

	
		and Stephan Günnemann
	

<br>
<span class="font-italic">Advances in Neural Information Processing Systems</span>, NeurIPS 2022.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2022randomized" aria-expanded="false"  aria-controls="scholten2022randomized">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/interception-smoothing/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://www.youtube.com/watch?v=rbFiXrh9Snk" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/interception_smoothing" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{scholten2022randomized,
  author = {Scholten, Yan and Schuchardt, Jan and Geisler, Simon and Bojchevski, Aleksandar and Günnemann, Stephan},
  title = {Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  organization = {Advances in Neural Information Processing Systems},
  year = {2022},
  url = {https://openreview.net/forum?id=t0VbBTw-o8},
  abbr = {NeurIPS},
  pdf = {https://yascho.github.io/assets/pdf/scholten2022randomized.pdf},
  code = {https://github.com/yascho/interception_smoothing},
  talk = {https://www.youtube.com/watch?v=rbFiXrh9Snk},
  slides = {https://yascho.github.io/assets/pdf/scholten2022randomized-slides.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2022randomized-poster.pdf},
  web = {https://www.cs.cit.tum.de/daml/interception-smoothing/},
  abs = {Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2022randomized">
	<div class="card card-body">
		Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf">CauseNet: Towards a Causality Graph Extracted from the Web</a>
<br>
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Henning Wachsmuth,
	

	
		Axel-Cyrille Ngonga Ngomo,
	

	
		and Martin Potthast
	

<br>
<span class="font-italic">International Conference on Information and Knowledge Management</span>, CIKM 2020.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#heindorf2020causenet" aria-expanded="false"  aria-controls="heindorf2020causenet">Abs</a>
	
	
		<a role="button" href="https://causenet.org/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://www.youtube.com/watch?v=aza77qJeZBo" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
	
	
		<a role="button" href="https://github.com/causenet-org/CIKM-20" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{heindorf2020causenet,
  author = {Heindorf, Stefan and Scholten, Yan and Wachsmuth, Henning and Ngomo, Axel{-}Cyrille Ngonga and Potthast, Martin},
  title = {CauseNet: Towards a Causality Graph Extracted from the Web},
  booktitle = {International Conference on Information and Knowledge Management},
  organization = {International Conference on Information and Knowledge Management},
  abbr = {CIKM},
  pages = {3023--3030},
  publisher = {{ACM}},
  year = {2020},
  web = {https://causenet.org/},
  pdf = {https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf},
  code = {https://github.com/causenet-org/CIKM-20},
  talk = {https://www.youtube.com/watch?v=aza77qJeZBo},
  abs = {Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2020causenet">
	<div class="card card-body">
		Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://webis.de/downloads/publications/papers/heindorf_2019.pdf">Debiasing Vandalism Detection Models at Wikidata</a>
<br>
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Gregor Engels,
	

	
		and Martin Potthast
	

<br>
<span class="font-italic">International World Wide Web Conference</span>, WWW 2019.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#heindorf2019debiasing" aria-expanded="false"  aria-controls="heindorf2019debiasing">Abs</a>
	
	
		<a role="button" href="https://www.heindorf.me/wdvd.html" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://webis.de/downloads/publications/papers/heindorf_2019.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
		<a role="button" href="https://github.com/heindorf/www19-fair-classification" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{heindorf2019debiasing,
  author = {Heindorf, Stefan and Scholten, Yan and Engels, Gregor and Potthast, Martin},
  title = {Debiasing Vandalism Detection Models at Wikidata},
  booktitle = {International World Wide Web Conference},
  organization = {International World Wide Web Conference},
  abbr = {WWW},
  pages = {670--680},
  publisher = {{ACM}},
  year = {2019},
  web = {https://www.heindorf.me/wdvd.html},
  pdf = {https://webis.de/downloads/publications/papers/heindorf_2019.pdf},
  code = {https://github.com/heindorf/www19-fair-classification},
  abs = {Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC AUC and 0.316 PR AUC.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay='{"show":"500"}' data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2019debiasing">
	<div class="card card-body">
		Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC AUC and 0.316 PR AUC.
	</div>
</div>
</div></li></ol>
</div>
-->

<div class="container pe-0 ps-0 ps-md-3">

  <h2>2025</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/pdf/2507.04219">Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs</a>
<br />
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Sophie Xhonneux,
	

	
		Leo Schwinn*,
	

	
		and Stephan Günnemann*
	

<br />
<span class="font-italic">arXiv preprint</span>,  2025.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2025model" aria-expanded="false" aria-controls="scholten2025model">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/partial-model-collapse/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://arxiv.org/pdf/2507.04219" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
		<a role="button" href="https://github.com/yascho/partial-model-collapse" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@misc{scholten2025model,
  title = {Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs},
  author = {Scholten, Yan and Xhonneux, Sophie and Schwinn*, Leo and Günnemann*, Stephan},
  year = {2025},
  eprint = {2507.04219},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
  url = {https://arxiv.org/abs/2507.04219},
  pdf = {https://arxiv.org/pdf/2507.04219},
  organization = {arXiv preprint},
  web = {https://www.cs.cit.tum.de/daml/partial-model-collapse/},
  code = {https://github.com/yascho/partial-model-collapse},
  abs = {Current unlearning methods for LLMs optimize on the private information they seek to remove by incorporating it into their training objectives. We argue this not only risks reinforcing exposure to sensitive data, it also fundamentally contradicts the principle of minimizing its use. As a remedy, we propose a novel unlearning method - Partial Model Collapse (PMC), which does not require unlearning targets in the unlearning objective. Our approach is inspired by recent observations that training generative models on their own generations leads to distribution collapse, effectively removing information from the model. Our core idea is to leverage this collapse for unlearning by triggering collapse partially on the sensitive data. We theoretically analyze that our approach converges to the desired outcome, i.e. the LLM unlearns the information in the forget set. We empirically demonstrate that PMC overcomes two key limitations of existing unlearning approaches that explicitly optimize on unlearning targets, and more effectively removes private information from model outputs. Overall, our contributions represent an important step toward more comprehensive unlearning that aligns with real-world privacy constraints.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2025model">
	<div class="card card-body">
		Current unlearning methods for LLMs optimize on the private information they seek to remove by incorporating it into their training objectives. We argue this not only risks reinforcing exposure to sensitive data, it also fundamentally contradicts the principle of minimizing its use. As a remedy, we propose a novel unlearning method - Partial Model Collapse (PMC), which does not require unlearning targets in the unlearning objective. Our approach is inspired by recent observations that training generative models on their own generations leads to distribution collapse, effectively removing information from the model. Our core idea is to leverage this collapse for unlearning by triggering collapse partially on the sensitive data. We theoretically analyze that our approach converges to the desired outcome, i.e. the LLM unlearns the information in the forget set. We empirically demonstrate that PMC overcomes two key limitations of existing unlearning approaches that explicitly optimize on unlearning targets, and more effectively removes private information from model outputs. Overall, our contributions represent an important step toward more comprehensive unlearning that aligns with real-world privacy constraints.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/pdf/2507.04446">Tail-aware Adversarial Attacks: A Distributional Approach to Efficient LLM Jailbreaking</a>
<br />
<div class="container authors p-0">

	
		Tim Beyer,
	

	
		<b>Yan Scholten</b>,
	

	
		Leo Schwinn*,
	

	
		and Stephan Günnemann*
	

<br />
<span class="font-italic">arXiv preprint</span>,  2025.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#beyer2025tailawareadversarialattacksdistributional" aria-expanded="false" aria-controls="beyer2025tailawareadversarialattacksdistributional">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/tail-aware-attacks/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://arxiv.org/pdf/2507.04446" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
	
  	<a role="button" onclick="var txt=`@misc{beyer2025tailawareadversarialattacksdistributional,
  title = {Tail-aware Adversarial Attacks: A Distributional Approach to Efficient LLM Jailbreaking},
  author = {Beyer, Tim and Scholten, Yan and Schwinn*, Leo and Günnemann*, Stephan},
  year = {2025},
  eprint = {2507.04446},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
  url = {https://arxiv.org/abs/2507.04446},
  pdf = {https://arxiv.org/pdf/2507.04446},
  web = {https://www.cs.cit.tum.de/daml/tail-aware-attacks/},
  organization = {arXiv preprint},
  abs = {To guarantee safe and robust deployment of large language models (LLMs) at scale, it is critical to accurately assess their adversarial robustness. Existing adversarial attacks typically target harmful responses in single-point, greedy generations, overlooking the inherently stochastic nature of LLMs. In this paper, we propose a novel framework for adversarial robustness evaluation that explicitly models the entire output distribution, including tail-risks, providing better estimates for model robustness at scale. By casting the attack process as a resource allocation problem between optimization and sampling, we determine compute-optimal tradeoffs and show that integrating sampling into existing attacks boosts ASR by up to 48% and improves efficiency by up to two orders of magnitude. Our framework also enables us to analyze how different attack algorithms affect output harm distributions. Surprisingly, we find that most optimization strategies have little effect on output harmfulness. Finally, we introduce a data-free proof-of-concept objective based on entropy-maximization to demonstrate how our tail-aware perspective enables new optimization targets. Overall, our findings highlight the importance of tail-aware attacks and evaluation protocols to accurately assess and strengthen LLM safety.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="beyer2025tailawareadversarialattacksdistributional">
	<div class="card card-body">
		To guarantee safe and robust deployment of large language models (LLMs) at scale, it is critical to accurately assess their adversarial robustness. Existing adversarial attacks typically target harmful responses in single-point, greedy generations, overlooking the inherently stochastic nature of LLMs. In this paper, we propose a novel framework for adversarial robustness evaluation that explicitly models the entire output distribution, including tail-risks, providing better estimates for model robustness at scale. By casting the attack process as a resource allocation problem between optimization and sampling, we determine compute-optimal tradeoffs and show that integrating sampling into existing attacks boosts ASR by up to 48% and improves efficiency by up to two orders of magnitude. Our framework also enables us to analyze how different attack algorithms affect output harm distributions. Surprisingly, we find that most optimization strategies have little effect on output harmfulness. Finally, we introduce a data-free proof-of-concept objective based on entropy-maximization to demonstrate how our tail-aware perspective enables new optimization targets. Overall, our findings highlight the importance of tail-aware attacks and evaluation protocols to accurately assess and strengthen LLM safety.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2025probabilistic.pdf">A Probabilistic Perspective on Unlearning and Alignment for Large Language Models</a>
<br />
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Stephan Günnemann,
	

	
		and Leo Schwinn
	

<br />
<span class="font-italic">International Conference on Learning Representations</span>, ICLR 2025 (<strong class="acceptance_type">Oral</strong>).
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2025probabilistic" aria-expanded="false" aria-controls="scholten2025probabilistic">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/probabilistic-unlearning/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025probabilistic.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025probabilistic-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025probabilistic-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/probabilistic-unlearning" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{scholten2025probabilistic,
  author = {Scholten, Yan and Günnemann, Stephan and Schwinn, Leo},
  title = {A Probabilistic Perspective on Unlearning and Alignment for Large Language Models},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  year = {2025},
  url = {https://openreview.net/forum?id=51WraMid8K},
  organization = {International Conference on Learning Representations},
  abbr = {ICLR},
  web = {https://www.cs.cit.tum.de/daml/probabilistic-unlearning/},
  pdf = {https://yascho.github.io/assets/pdf/scholten2025probabilistic.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2025probabilistic-poster.pdf},
  slides = {https://yascho.github.io/assets/pdf/scholten2025probabilistic-slides.pdf},
  code = {https://github.com/yascho/probabilistic-unlearning},
  abs = {Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding inaccurate estimations of model capabilities. This is particularly problematic in critical contexts such as unlearning and alignment, where precise model evaluations are crucial. To remedy this, we introduce the first formal probabilistic evaluation framework for LLMs. Namely, we propose novel metrics with high probability guarantees concerning the output distribution of a model. Our metrics are application-independent and allow practitioners to make more reliable estimates about model capabilities before deployment. Our experimental analysis reveals that deterministic evaluations falsely indicate successful unlearning and alignment, whereas our probabilistic evaluations better capture model capabilities. We show how to overcome challenges associated with probabilistic outputs in a case study on unlearning by introducing (1) a novel loss based on entropy optimization, and (2) adaptive temperature scaling. We demonstrate that our approach significantly enhances unlearning in probabilistic settings on recent benchmarks. Overall, our proposed shift from point estimates to probabilistic evaluations of output distributions represents an important step toward comprehensive evaluations of LLMs.},
  howpublished = {Oral}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2025probabilistic">
	<div class="card card-body">
		Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding inaccurate estimations of model capabilities. This is particularly problematic in critical contexts such as unlearning and alignment, where precise model evaluations are crucial. To remedy this, we introduce the first formal probabilistic evaluation framework for LLMs. Namely, we propose novel metrics with high probability guarantees concerning the output distribution of a model. Our metrics are application-independent and allow practitioners to make more reliable estimates about model capabilities before deployment. Our experimental analysis reveals that deterministic evaluations falsely indicate successful unlearning and alignment, whereas our probabilistic evaluations better capture model capabilities. We show how to overcome challenges associated with probabilistic outputs in a case study on unlearning by introducing (1) a novel loss based on entropy optimization, and (2) adaptive temperature scaling. We demonstrate that our approach significantly enhances unlearning in probabilistic settings on recent benchmarks. Overall, our proposed shift from point estimates to probabilistic evaluations of output distributions represents an important step toward comprehensive evaluations of LLMs.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2025provably.pdf">Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning</a>
<br />
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">International Conference on Learning Representations</span>, ICLR 2025 (<strong class="acceptance_type">Spotlight</strong>).
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2025provably" aria-expanded="false" aria-controls="scholten2025provably">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/reliable-conformal-prediction/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025provably.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2025provably-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/reliable_conformal_prediction" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{scholten2025provably,
  author = {Scholten, Yan and Günnemann, Stephan},
  title = {Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  year = {2025},
  url = {https://openreview.net/forum?id=ofuLWn8DFZ},
  organization = {International Conference on Learning Representations},
  abbr = {ICLR},
  pdf = {https://yascho.github.io/assets/pdf/scholten2025provably.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2025provably-poster.pdf},
  web = {https://www.cs.cit.tum.de/daml/reliable-conformal-prediction/},
  code = {https://github.com/yascho/reliable_conformal_prediction},
  abs = {Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning.},
  howpublished = {Spotlight}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2025provably">
	<div class="card card-body">
		Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/pdf/2502.11910">Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives</a>
<br />
<div class="container authors p-0">

	
		Leo Schwinn,
	

	
		<b>Yan Scholten</b>,
	

	
		Tom Wollschläger,
	

	
		Sophie Xhonneux,
	

	
		Stephen Casper,
	

	
		Stephan Günnemann,
	

	
		and Gauthier Gidel
	

<br />
<span class="font-italic">arXiv preprint</span>,  2025.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#schwinn2025adversarial" aria-expanded="false" aria-controls="schwinn2025adversarial">Abs</a>
	
	
	
		<a role="button" href="https://arxiv.org/pdf/2502.11910" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
	
  	<a role="button" onclick="var txt=`@misc{schwinn2025adversarial,
  title = {Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives},
  author = {Schwinn, Leo and Scholten, Yan and Wollschläger, Tom and Xhonneux, Sophie and Casper, Stephen and Günnemann, Stephan and Gidel, Gauthier},
  year = {2025},
  eprint = {2502.11910},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
  url = {https://arxiv.org/abs/2502.11910},
  pdf = {https://arxiv.org/pdf/2502.11910},
  organization = {arXiv preprint},
  abs = {Misaligned research objectives have considerably hindered progress in adversarial robustness research over the past decade. For instance, an extensive focus on optimizing target metrics, while neglecting rigorous standardized evaluation, has led researchers to pursue ad-hoc heuristic defenses that were seemingly effective. Yet, most of these were exposed as flawed by subsequent evaluations, ultimately contributing little measurable progress to the field. In this position paper, we illustrate that current research on the robustness of large language models (LLMs) risks repeating past patterns with potentially worsened real-world implications. To address this, we argue that realigned objectives are necessary for meaningful progress in adversarial alignment. To this end, we build on established cybersecurity taxonomy to formally define differences between past and emerging threat models that apply to LLMs. Using this framework, we illustrate that progress requires disentangling adversarial alignment into addressable sub-problems and returning to core academic principles, such as measureability, reproducibility, and comparability. Although the field presents significant challenges, the fresh start on adversarial robustness offers the unique opportunity to build on past experience while avoiding previous mistakes.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="schwinn2025adversarial">
	<div class="card card-body">
		Misaligned research objectives have considerably hindered progress in adversarial robustness research over the past decade. For instance, an extensive focus on optimizing target metrics, while neglecting rigorous standardized evaluation, has led researchers to pursue ad-hoc heuristic defenses that were seemingly effective. Yet, most of these were exposed as flawed by subsequent evaluations, ultimately contributing little measurable progress to the field. In this position paper, we illustrate that current research on the robustness of large language models (LLMs) risks repeating past patterns with potentially worsened real-world implications. To address this, we argue that realigned objectives are necessary for meaningful progress in adversarial alignment. To this end, we build on established cybersecurity taxonomy to formally define differences between past and emerging threat models that apply to LLMs. Using this framework, we illustrate that progress requires disentangling adversarial alignment into addressable sub-problems and returning to core academic principles, such as measureability, reproducibility, and comparability. Although the field presents significant challenges, the fresh start on adversarial robustness offers the unique opportunity to build on past experience while avoiding previous mistakes.
	</div>
</div>
</div></li></ol>
  </div>

  <h2>2024</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://arxiv.org/abs/2310.04285">Assessing Robustness via Score-Based Adversarial Image Generation</a>
<br />
<div class="container authors p-0">

	
		Marcel Kollovieh,
	

	
		Lukas Gosch,
	

	
		Marten Lienen,
	

	
		<b>Yan Scholten</b>,
	

	
		Leo Schwinn,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">Transactions on Machine Learning Research</span>, TMLR 2024.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#kollovieh2024assessing" aria-expanded="false" aria-controls="kollovieh2024assessing">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/scoreag/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://arxiv.org/abs/2310.04285" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
	
  	<a role="button" onclick="var txt=`@inproceedings{kollovieh2024assessing,
  title = {Assessing Robustness via Score-Based Adversarial Image Generation},
  author = {Kollovieh, Marcel and Gosch, Lukas and Lienen, Marten and Scholten, Yan and Schwinn, Leo and Günnemann, Stephan},
  booktitle = {Transactions on Machine Learning Research},
  organization = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  year = {2024},
  url = {https://openreview.net/forum?id=7Oqb6zlGWl},
  abbr = {TMLR},
  pdf = {https://arxiv.org/abs/2310.04285},
  web = {https://www.cs.cit.tum.de/daml/scoreag/},
  abs = {Most adversarial attacks and defenses focus on perturbations within small lp-norm constraints. However, lp threat models cannot capture all relevant semantics-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate unrestricted adversarial examples that overcome the limitations of lp-norm constraints. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG improves upon the majority of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than lp-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="kollovieh2024assessing">
	<div class="card card-body">
		Most adversarial attacks and defenses focus on perturbations within small lp-norm constraints. However, lp threat models cannot capture all relevant semantics-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate unrestricted adversarial examples that overcome the limitations of lp-norm constraints. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG improves upon the majority of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than lp-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments.
	</div>
</div>
</div></li></ol>
  </div>

  <h2>2023</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf">Hierarchical Randomized Smoothing</a>
<br />
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Jan Schuchardt,
	

	
		Aleksandar Bojchevski,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">Advances in Neural Information Processing Systems</span>, NeurIPS 2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2023hierarchical" aria-expanded="false" aria-controls="scholten2023hierarchical">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/hierarchical-smoothing/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://nips.cc/virtual/2023/poster/72764" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2023hierarchical-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/hierarchical_smoothing" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{scholten2023hierarchical,
  author = {Scholten, Yan and Schuchardt, Jan and Bojchevski, Aleksandar and Günnemann, Stephan},
  title = {Hierarchical Randomized Smoothing},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year = {2023},
  url = {https://openreview.net/forum?id=6IhNHKyuJO},
  organization = {Advances in Neural Information Processing Systems},
  abbr = {NeurIPS},
  slides = {https://yascho.github.io/assets/pdf/scholten2023hierarchical-slides.pdf},
  web = {https://www.cs.cit.tum.de/daml/hierarchical-smoothing/},
  code = {https://github.com/yascho/hierarchical_smoothing},
  talk = {https://nips.cc/virtual/2023/poster/72764},
  pdf = {https://yascho.github.io/assets/pdf/scholten2023hierarchical.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2023hierarchical-poster.pdf},
  abs = {Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs -- by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both -- certifiably robust to perturbations and accurate.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2023hierarchical">
	<div class="card card-body">
		Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs – by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both – certifiably robust to perturbations and accurate.
	</div>
</div>
</div></li>
<li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://openreview.net/pdf?id=mLe63bAYc7">Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More</a>
<br />
<div class="container authors p-0">

	
		Jan Schuchardt,
	

	
		<b>Yan Scholten</b>,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">Advances in Neural Information Processing Systems</span>, NeurIPS 2023.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#schuchardt2023provable" aria-expanded="false" aria-controls="schuchardt2023provable">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/equivariance-robustness/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://openreview.net/pdf?id=mLe63bAYc7" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://nips.cc/virtual/2023/poster/70555" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
	
		<a role="button" href="https://nips.cc/media/PosterPDFs/NeurIPS%202023/70555.png?t=1701529700.1135688" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/jan-schuchardt/group_equivariance_robustness" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{schuchardt2023provable,
  author = {Schuchardt, Jan and Scholten, Yan and Günnemann, Stephan},
  title = {Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year = {2023},
  url = {https://openreview.net/forum?id=mLe63bAYc7},
  organization = {Advances in Neural Information Processing Systems},
  abbr = {NeurIPS},
  web = {https://www.cs.cit.tum.de/daml/equivariance-robustness/},
  poster = {https://nips.cc/media/PosterPDFs/NeurIPS%202023/70555.png?t=1701529700.1135688},
  talk = {https://nips.cc/virtual/2023/poster/70555},
  pdf = {https://openreview.net/pdf?id=mLe63bAYc7},
  code = {https://github.com/jan-schuchardt/group_equivariance_robustness},
  abs = {A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="schuchardt2023provable">
	<div class="card card-body">
		A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input’s semantic content. Furthermore, there are perturbations for which a model’s prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task’s equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.
	</div>
</div>
</div></li></ol>
  </div>

  <h2>2022</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://yascho.github.io/assets/pdf/scholten2022randomized.pdf">Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks</a>
<br />
<div class="container authors p-0">

	
		<b>Yan Scholten</b>,
	

	
		Jan Schuchardt,
	

	
		Simon Geisler,
	

	
		Aleksandar Bojchevski,
	

	
		and Stephan Günnemann
	

<br />
<span class="font-italic">Advances in Neural Information Processing Systems</span>, NeurIPS 2022.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#scholten2022randomized" aria-expanded="false" aria-controls="scholten2022randomized">Abs</a>
	
	
		<a role="button" href="https://www.cs.cit.tum.de/daml/interception-smoothing/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://www.youtube.com/watch?v=rbFiXrh9Snk" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized-slides.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Slides</a>
	
	
		<a role="button" href="https://yascho.github.io/assets/pdf/scholten2022randomized-poster.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Poster</a>
	
	
		<a role="button" href="https://github.com/yascho/interception_smoothing" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{scholten2022randomized,
  author = {Scholten, Yan and Schuchardt, Jan and Geisler, Simon and Bojchevski, Aleksandar and Günnemann, Stephan},
  title = {Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  organization = {Advances in Neural Information Processing Systems},
  year = {2022},
  url = {https://openreview.net/forum?id=t0VbBTw-o8},
  abbr = {NeurIPS},
  pdf = {https://yascho.github.io/assets/pdf/scholten2022randomized.pdf},
  code = {https://github.com/yascho/interception_smoothing},
  talk = {https://www.youtube.com/watch?v=rbFiXrh9Snk},
  slides = {https://yascho.github.io/assets/pdf/scholten2022randomized-slides.pdf},
  poster = {https://yascho.github.io/assets/pdf/scholten2022randomized-poster.pdf},
  web = {https://www.cs.cit.tum.de/daml/interception-smoothing/},
  abs = {Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="scholten2022randomized">
	<div class="card card-body">
		Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.
	</div>
</div>
</div></li></ol>
  </div>

  <h2>2020</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf">CauseNet: Towards a Causality Graph Extracted from the Web</a>
<br />
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Henning Wachsmuth,
	

	
		Axel-Cyrille Ngonga Ngomo,
	

	
		and Martin Potthast
	

<br />
<span class="font-italic">International Conference on Information and Knowledge Management</span>, CIKM 2020.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#heindorf2020causenet" aria-expanded="false" aria-controls="heindorf2020causenet">Abs</a>
	
	
		<a role="button" href="https://causenet.org/" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
		<a role="button" href="https://www.youtube.com/watch?v=aza77qJeZBo" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Talk</a>
	
	
	
	
		<a role="button" href="https://github.com/causenet-org/CIKM-20" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{heindorf2020causenet,
  author = {Heindorf, Stefan and Scholten, Yan and Wachsmuth, Henning and Ngomo, Axel{-}Cyrille Ngonga and Potthast, Martin},
  title = {CauseNet: Towards a Causality Graph Extracted from the Web},
  booktitle = {International Conference on Information and Knowledge Management},
  organization = {International Conference on Information and Knowledge Management},
  abbr = {CIKM},
  pages = {3023--3030},
  publisher = {{ACM}},
  year = {2020},
  web = {https://causenet.org/},
  pdf = {https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf},
  code = {https://github.com/causenet-org/CIKM-20},
  talk = {https://www.youtube.com/watch?v=aza77qJeZBo},
  abs = {Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2020causenet">
	<div class="card card-body">
		Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.
	</div>
</div>
</div></li></ol>
  </div>

  <h2>2019</h2>
  <div class="container">
  	<ol class="bibliography"><li><div class="container mt-2 mb-3 ps-0 pe-0">
<a class="title" href="https://webis.de/downloads/publications/papers/heindorf_2019.pdf">Debiasing Vandalism Detection Models at Wikidata</a>
<br />
<div class="container authors p-0">

	
		Stefan Heindorf,
	

	
		<b>Yan Scholten</b>,
	

	
		Gregor Engels,
	

	
		and Martin Potthast
	

<br />
<span class="font-italic">International World Wide Web Conference</span>, WWW 2019.
</div>
<div class="btn-group pt-1 flex-wrap" role="group">
	
	<a class="btn btn-primary btn-sm collapsed" role="button" data-bs-toggle="collapse" data-bs-target="#heindorf2019debiasing" aria-expanded="false" aria-controls="heindorf2019debiasing">Abs</a>
	
	
		<a role="button" href="https://www.heindorf.me/wdvd.html" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Web</a>
	
	
		<a role="button" href="https://webis.de/downloads/publications/papers/heindorf_2019.pdf" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">PDF</a>
	
	
	
	
	
		<a role="button" href="https://github.com/heindorf/www19-fair-classification" class="btn btn-primary btn-sm collapsed" target="_blank" rel="noopener noreferrer">Code</a>
	
	
  	<a role="button" onclick="var txt=`@inproceedings{heindorf2019debiasing,
  author = {Heindorf, Stefan and Scholten, Yan and Engels, Gregor and Potthast, Martin},
  title = {Debiasing Vandalism Detection Models at Wikidata},
  booktitle = {International World Wide Web Conference},
  organization = {International World Wide Web Conference},
  abbr = {WWW},
  pages = {670--680},
  publisher = {{ACM}},
  year = {2019},
  web = {https://www.heindorf.me/wdvd.html},
  pdf = {https://webis.de/downloads/publications/papers/heindorf_2019.pdf},
  code = {https://github.com/heindorf/www19-fair-classification},
  abs = {Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC AUC and 0.316 PR AUC.}
}
`; txt = txt.replace(/\b(organization|abbr|web|pdf|code|howpublished|abs|slides|talk|poster)\s*=\s*\{.*?\},?\s*\n?/gm, ''); txt = txt.replace(/^\s*}/gm, '}'); copy(txt);" class="btn btn-primary btn-sm collapsed" data-toggle="tooltip" data-delay="{&quot;show&quot;:&quot;500&quot;}" data-placement="top" title="Copied!">BibTeX</a>
	
</div>
<div class="collapse mt-1 p-0" id="heindorf2019debiasing">
	<div class="card card-body">
		Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC AUC and 0.316 PR AUC.
	</div>
</div>
</div></li></ol>
  </div>

</div>
</article>
    </main>
	  <footer class="footer m-auto mt-5 p-0">
	<div class="d-flex p-0">
		<div class="flex-fill text-start p-0">
			<span class="text-muted">Yan Scholten &copy; <script>document.write(/\d{4}/.exec(Date())[0])</script>
</span>
		</div>
		<div class="flex-fill text-end p-0">
			<span class="text-muted">Last updated: January 22, 2025.</span>
		</div>
	</div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    <script src="/js/copy.js"></script>
    <script>
      $(function () {
      $('[data-toggle="tooltip"]').tooltip({
      animated: 'fade',
      trigger: 'click',
      });
    });
    $("[data-toggle='tooltip']").on('mouseleave', function(){
      setTimeout(function() {   //calls click event after a certain time
       $('[data-toggle="tooltip"]').tooltip('hide');
      }, 1000);
    });
    </script>
  </body>
</html>